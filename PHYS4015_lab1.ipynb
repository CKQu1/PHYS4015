{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import scipy.io as sio\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "type(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.2725, 0.9603],\n",
      "        [0.5393, 0.7805]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes\n",
    "shape = (2,3,4)\n",
    "rand_tensor = torch.rand(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8446, 0.7858, 0.0075, 0.3817],\n",
      "         [0.2382, 0.8923, 0.7469, 0.4160],\n",
      "         [0.4269, 0.2646, 0.4014, 0.4053]],\n",
      "\n",
      "        [[0.1034, 0.4249, 0.6453, 0.7485],\n",
      "         [0.7507, 0.3386, 0.5691, 0.1645],\n",
      "         [0.9479, 0.6755, 0.6040, 0.5971]]])\n"
     ]
    }
   ],
   "source": [
    "print(rand_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor operations\n",
    "tensor = torch.ones(4,4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim = 1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4., 4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "a = torch.ones((1,4))\n",
    "b = torch.ones((4,4))\n",
    "c = torch.mm(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "a = torch.ones((4,))\n",
    "b = torch.ones((4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 4., 4., 4.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1,3,64,64)\n",
    "labels = torch.rand(1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-6.2734e-01, -4.9248e-01, -6.4502e-01, -1.6093e+00, -8.6458e-01,\n",
       "         -3.6988e-01, -5.4519e-01,  4.9333e-01,  4.5190e-01, -7.7892e-01,\n",
       "         -7.3775e-01, -8.5429e-01, -3.9555e-01, -8.4611e-01, -1.1456e+00,\n",
       "         -5.7683e-01, -9.7403e-01, -3.5067e-01, -6.1748e-01, -4.2001e-01,\n",
       "         -1.5213e+00, -8.3086e-01, -1.4875e+00,  2.6009e-01, -9.4014e-01,\n",
       "         -1.2512e+00, -7.3821e-01, -9.9055e-01, -1.0611e+00, -3.5616e-01,\n",
       "         -8.9132e-01, -9.6220e-01, -5.5745e-01, -4.2756e-01, -8.2799e-02,\n",
       "         -4.1557e-01,  6.8197e-01, -7.1923e-01, -4.2995e-01,  2.7831e-01,\n",
       "         -6.8829e-01, -6.9781e-01, -1.1050e+00, -2.6512e-01, -6.4584e-01,\n",
       "         -3.6983e-01, -7.2499e-01, -3.4136e-01, -1.0995e+00, -1.0657e+00,\n",
       "         -3.6238e-01,  5.6677e-01, -3.0998e-01, -7.7016e-01, -9.3642e-02,\n",
       "         -1.1471e+00, -1.4939e-01, -1.3187e+00, -6.2178e-01, -3.7587e-01,\n",
       "          8.3733e-01,  3.1180e-01,  1.1813e-01,  1.1110e-01, -6.7253e-01,\n",
       "         -3.8787e-01, -2.0832e-01, -4.0028e-01, -7.8485e-01, -1.0769e+00,\n",
       "         -1.8296e+00,  6.4186e-02, -1.6357e+00, -4.0035e-01, -1.4167e+00,\n",
       "         -1.5956e+00,  1.0159e-01, -6.1235e-01,  1.3100e-01, -8.5858e-02,\n",
       "         -8.9534e-01, -1.6018e+00, -3.6120e-02, -8.3139e-01, -6.2061e-01,\n",
       "         -7.6406e-02,  3.8694e-02,  2.8928e-01, -1.7868e-01, -6.8610e-01,\n",
       "         -1.4514e+00, -1.0532e+00, -2.2309e+00, -3.6738e-01,  1.8583e-01,\n",
       "         -2.1680e+00, -8.8139e-01, -6.1446e-01, -1.3302e+00, -1.3140e-01,\n",
       "         -1.2400e+00, -8.1403e-01, -8.0171e-01, -5.3674e-02, -5.1527e-03,\n",
       "         -3.0358e-01, -2.3942e-01, -1.5390e+00, -1.0634e+00, -1.5299e+00,\n",
       "         -1.3083e+00, -5.4323e-01,  1.0990e+00,  2.5244e-01,  2.6364e-01,\n",
       "         -1.1758e+00, -8.3707e-01, -4.6057e-01,  4.0347e-01, -4.2424e-01,\n",
       "         -9.2636e-01, -2.9882e-01,  3.2544e-01, -1.3398e-01,  7.7084e-01,\n",
       "         -3.9498e-01,  2.0478e-01, -1.4033e+00, -1.2460e+00, -1.2889e+00,\n",
       "         -1.3156e+00, -1.6919e+00, -1.2023e+00, -1.4206e+00, -8.5144e-01,\n",
       "         -1.1856e+00, -9.6771e-01, -1.3903e+00, -1.4103e+00, -1.3544e+00,\n",
       "         -1.4417e+00, -1.7403e+00, -2.1671e+00, -1.6719e+00, -3.9543e-01,\n",
       "         -7.1461e-01, -8.9629e-01, -1.7249e+00, -1.2688e+00, -1.1625e+00,\n",
       "          3.1521e-01,  1.5647e+00, -1.1018e+00, -4.9861e-01,  1.0094e-02,\n",
       "          2.5882e-01, -4.2427e-01, -1.5898e-01,  2.5577e-02,  4.3784e-01,\n",
       "          4.2555e-01,  5.8905e-01,  1.6960e-01,  7.4247e-01,  3.3408e-01,\n",
       "         -2.0993e-01, -2.1117e-01, -4.0602e-01,  7.1645e-01, -1.2090e-01,\n",
       "          1.4671e-03,  6.8735e-01,  3.7729e-01,  3.5266e-01,  2.6686e-01,\n",
       "         -5.1290e-01,  9.1485e-02,  1.4303e-01,  5.5163e-01,  5.3531e-01,\n",
       "          5.6533e-01,  3.5457e-02,  5.5856e-01,  1.2028e-01,  6.0897e-01,\n",
       "          6.5096e-01,  6.1853e-01,  1.7281e-01,  2.7077e-01,  6.9878e-01,\n",
       "         -3.3600e-01,  3.5159e-01,  3.0433e-01,  3.7373e-01, -6.0274e-01,\n",
       "          5.2848e-01, -5.0117e-02,  1.4078e-01,  7.1636e-02,  6.4849e-01,\n",
       "          3.3994e-01,  2.5129e-01,  7.2030e-01,  4.3228e-01,  2.8319e-01,\n",
       "          3.6190e-01,  1.3946e-01,  7.8110e-01,  1.4020e+00,  6.0584e-01,\n",
       "         -1.2269e-01,  6.0795e-01,  7.4064e-01,  7.0294e-03,  1.0132e-01,\n",
       "          4.9224e-01,  1.5485e-01,  3.8906e-01, -2.1367e-01,  7.7503e-01,\n",
       "          1.5752e-01, -1.1905e-01,  2.0641e-01,  5.7196e-01,  2.1363e-01,\n",
       "          5.0444e-01,  2.8253e-01,  8.0346e-01, -3.4838e-01, -4.6078e-04,\n",
       "          3.6690e-02,  7.0766e-01,  4.7076e-01, -5.7078e-02,  6.0604e-01,\n",
       "          8.6794e-01,  2.9392e-01,  1.2294e-01,  5.9715e-01, -9.7705e-02,\n",
       "          5.3288e-01, -2.7227e-01,  3.8549e-01,  4.2129e-01, -2.1146e-01,\n",
       "          3.4686e-01,  4.5335e-01,  1.8989e-01,  6.4542e-01,  3.9685e-01,\n",
       "          3.6714e-01,  5.2791e-01, -6.5355e-01,  4.9141e-01,  6.8548e-01,\n",
       "         -5.8363e-01,  5.7722e-01,  4.2136e-01, -6.9602e-02,  2.6477e-01,\n",
       "         -1.4756e-01, -5.5204e-01, -4.1003e-01,  4.2538e-01,  8.0867e-01,\n",
       "          6.6788e-01,  2.8425e-01,  7.1036e-01,  3.1723e-02, -2.6234e-01,\n",
       "         -7.2496e-01, -9.1278e-01, -4.5367e-01,  8.4480e-01, -9.0792e-01,\n",
       "         -9.6139e-01, -1.0577e+00, -5.2291e-01, -9.9893e-01, -3.5958e-01,\n",
       "         -1.4542e-01,  7.5064e-01,  8.3512e-01, -6.5040e-02,  4.6103e-01,\n",
       "          7.4364e-01, -3.5869e-01, -5.0312e-01, -7.6000e-01, -1.6817e+00,\n",
       "         -1.1530e+00, -1.2370e+00, -4.1399e-01, -1.1423e+00, -9.9276e-01,\n",
       "         -8.7242e-01, -8.3357e-01, -1.2059e+00, -3.2986e-01, -3.4445e-01,\n",
       "         -2.1281e+00, -7.9230e-01, -6.1181e-01, -4.3311e-01, -1.2679e+00,\n",
       "         -1.0731e+00, -1.5630e-01, -1.0529e+00, -1.5438e+00, -6.4149e-01,\n",
       "          3.8402e-01, -6.3333e-01, -6.0310e-01,  1.6756e-01,  4.3862e-01,\n",
       "         -5.0247e-01, -8.8003e-01, -1.2836e+00, -1.6328e+00, -1.1420e+00,\n",
       "         -1.9210e+00, -1.5829e+00, -1.4925e+00, -1.7496e+00, -1.7582e+00,\n",
       "         -1.7379e+00, -1.7955e+00,  8.6666e-02, -1.8546e-01, -7.1560e-01,\n",
       "          2.6530e-02, -2.1971e-01,  1.8413e-01,  3.5174e-01, -2.5438e-01,\n",
       "         -7.0598e-01, -1.3520e+00,  2.9723e-01,  8.0837e-01, -1.1873e+00,\n",
       "         -7.1490e-01,  1.0964e+00, -1.9185e-01, -1.3327e+00, -6.8009e-01,\n",
       "          5.3093e-01, -7.7547e-01, -1.6797e+00, -1.3651e-01, -1.3956e+00,\n",
       "         -1.3848e+00, -2.4803e+00, -1.6477e+00, -1.0657e+00, -8.4076e-01,\n",
       "          4.2459e-01,  1.2210e+00,  1.6258e-01,  7.0397e-01,  5.6714e-01,\n",
       "         -6.6777e-02,  9.0630e-01,  6.8185e-02,  1.9345e-01, -5.5743e-01,\n",
       "         -4.7125e-01, -1.2006e+00, -3.6864e-01, -9.5380e-01, -6.5606e-01,\n",
       "         -5.9838e-01, -2.3535e-01, -4.7594e-01,  4.6068e-03, -1.8972e-01,\n",
       "         -1.0614e+00, -1.1477e+00,  3.6363e-01, -1.9815e-01, -4.8929e-01,\n",
       "          3.7207e-01, -3.4660e-01, -3.0661e-01, -7.3253e-01, -7.0256e-01,\n",
       "         -4.7429e-01, -1.0536e+00, -1.1185e+00, -8.6470e-01, -3.5426e-01,\n",
       "          5.6917e-01, -1.0137e-01, -1.5849e+00, -1.6528e+00, -4.6653e-01,\n",
       "          1.2794e-01, -1.4493e+00, -1.0593e+00,  7.3772e-01, -1.0634e-01,\n",
       "         -7.2333e-01,  9.3119e-01,  4.0164e-01, -2.2931e+00, -1.7830e+00,\n",
       "         -8.2722e-01, -2.3096e-01, -3.8027e-01, -2.8320e-01,  1.1750e+00,\n",
       "         -2.3808e-01,  2.5144e-01,  2.0474e+00,  7.5861e-01,  3.4177e-01,\n",
       "          6.8979e-01, -2.8793e-01,  2.1974e-01,  2.1529e-01,  1.2038e+00,\n",
       "          8.1583e-01,  1.2041e+00, -6.7311e-02,  1.4566e-01,  1.2411e-01,\n",
       "         -8.4865e-01,  3.3177e-01,  1.6165e+00,  1.5869e+00,  3.6070e-01,\n",
       "         -7.5894e-01, -1.7772e-01,  4.4331e-01,  4.0800e-01,  6.7660e-01,\n",
       "          9.3687e-01, -3.3654e-01, -3.0885e-01,  2.6895e-01,  4.7359e-01,\n",
       "          1.1591e+00,  6.8489e-01,  3.1247e-01, -6.1469e-01, -5.2925e-01,\n",
       "          4.0885e-01,  1.0354e-01,  1.7214e+00,  1.2289e+00, -5.0524e-01,\n",
       "         -1.4704e-01,  2.8459e-01,  4.6541e-01, -1.4737e-01, -1.4871e-01,\n",
       "          5.4664e-01,  1.4753e+00,  1.3506e+00,  2.3949e-02,  8.9912e-01,\n",
       "         -8.1870e-01,  6.3200e-01,  1.2796e+00,  2.5919e+00,  1.2901e+00,\n",
       "         -3.7310e-01, -1.6294e+00, -2.9787e-01, -1.7233e-01,  1.4210e+00,\n",
       "          1.1954e+00,  8.5935e-01,  2.8819e-01,  1.2015e+00, -3.1852e-01,\n",
       "         -2.1171e-01,  2.5645e-01,  5.0764e-01,  9.3170e-01,  4.5277e-01,\n",
       "          1.6856e-01,  2.4047e-01,  4.3054e-01, -8.1287e-01, -1.9429e+00,\n",
       "         -9.6572e-02,  3.6530e-02,  1.1263e+00,  1.5772e+00,  1.0525e+00,\n",
       "          9.7473e-01,  9.7722e-01,  7.7137e-01, -1.1749e+00,  1.6744e+00,\n",
       "         -8.9282e-01,  2.1268e-01, -4.6281e-01, -4.2247e-01,  1.2729e+00,\n",
       "         -1.6378e+00,  5.5990e-01,  1.4996e+00,  6.1099e-01,  1.0567e+00,\n",
       "          1.2094e+00,  8.9723e-01,  7.4473e-01,  4.0211e-01,  4.5676e-01,\n",
       "         -1.2970e+00, -9.8785e-01,  6.9748e-01,  4.9005e-01,  1.2493e+00,\n",
       "          1.7210e+00,  4.0977e-01,  1.4461e-01,  1.5241e+00,  7.4940e-01,\n",
       "         -6.3574e-01,  3.5135e-01,  1.1205e+00,  1.7128e+00,  2.5084e-01,\n",
       "         -7.1792e-01, -2.4699e-01, -5.0926e-01,  6.0210e-01,  1.5967e-01,\n",
       "          9.7783e-01,  5.0727e-01, -1.8290e-02, -1.0104e+00,  3.3004e-01,\n",
       "         -4.5540e-01, -4.5341e-01, -7.2562e-01, -4.9345e-02,  1.1972e+00,\n",
       "         -1.1891e+00,  1.7895e+00,  1.1330e+00,  1.0785e+00,  4.4845e-01,\n",
       "          6.8042e-01,  5.3623e-01, -2.0232e+00, -1.3065e+00, -2.3076e-01,\n",
       "         -4.3969e-01,  2.1359e-01,  6.1026e-01, -2.1910e-01, -1.4431e+00,\n",
       "         -5.3534e-01,  2.4144e-01,  2.2660e-01,  1.3936e+00,  9.4765e-01,\n",
       "          1.3335e-01,  1.8802e-02,  7.9686e-01,  2.3347e-01, -1.2717e+00,\n",
       "         -8.6542e-01,  5.4480e-01,  8.5400e-01,  4.3990e-01, -7.1017e-01,\n",
       "          8.7644e-01,  2.3379e-01,  9.7642e-01, -8.4996e-01,  4.6604e-01,\n",
       "         -9.1093e-02, -1.0077e+00,  1.3123e+00,  3.4110e-01,  2.6324e-01,\n",
       "          1.3096e-01, -2.4702e-01,  7.4707e-01,  9.7114e-01,  1.1196e+00,\n",
       "          9.9652e-01, -2.8147e-01,  1.8622e+00,  1.1186e+00,  1.3565e+00,\n",
       "         -6.0898e-01,  2.6424e-01, -7.3454e-01,  1.1428e+00,  7.4234e-01,\n",
       "         -5.9600e-01,  1.3350e+00, -2.5689e-05, -4.7465e-01,  7.9805e-01,\n",
       "          2.1085e+00, -2.0828e-01, -2.7398e-01, -6.4690e-01,  7.1630e-01,\n",
       "          1.9472e-01,  1.1568e+00, -4.7294e-01,  6.7733e-01, -1.9978e-01,\n",
       "          9.6942e-01,  1.0139e+00, -6.7146e-01,  6.1923e-01,  1.6167e-02,\n",
       "          4.2378e-01,  1.0249e+00,  4.1571e-01,  1.9404e+00,  9.8852e-01,\n",
       "          1.1732e+00,  6.4194e-01,  8.4327e-02,  4.3023e-01,  2.7131e-01,\n",
       "         -1.0739e+00,  1.2358e+00, -4.0318e-01, -1.3833e+00,  4.9994e-01,\n",
       "          1.5224e-01,  1.0027e+00,  6.3771e-01,  1.0281e+00,  1.3140e-01,\n",
       "          3.8614e-01,  9.6585e-01,  1.0148e+00,  7.8219e-01,  3.4471e-01,\n",
       "         -1.7070e+00,  1.2523e+00,  5.3216e-02,  1.4077e+00,  9.8466e-01,\n",
       "         -1.1288e+00,  5.5265e-01,  3.8977e-01, -8.0563e-01, -1.5149e+00,\n",
       "          1.0477e+00,  1.9766e-02,  8.0743e-01,  9.8823e-01, -8.2161e-02,\n",
       "          7.3666e-01, -5.6330e-02,  3.1113e-03,  8.7480e-03,  6.0195e-01,\n",
       "         -7.5659e-03, -1.0023e+00, -1.2012e-01, -7.7796e-01,  7.4058e-01,\n",
       "          1.3445e-01,  1.3423e+00,  4.8058e-01, -9.4326e-01, -6.1375e-01,\n",
       "          2.0708e-01, -3.1424e-01, -2.9832e-01,  3.4704e-01,  1.3515e+00,\n",
       "         -7.3888e-01,  1.6405e+00,  1.1910e+00,  9.5115e-01,  1.4107e-01,\n",
       "          7.7744e-01,  5.3698e-01, -6.4237e-01,  6.1989e-01,  1.1035e+00,\n",
       "         -1.5744e+00,  7.8279e-03, -7.4674e-01, -3.4594e-01, -8.7751e-01,\n",
       "         -7.0337e-01,  6.9136e-01,  9.7437e-01,  7.8043e-01, -9.1827e-01,\n",
       "          1.1105e+00,  1.7722e+00,  1.5995e-03, -3.4469e-01,  9.8642e-01,\n",
       "          1.8424e+00, -6.7499e-01, -3.9416e-01,  6.8112e-01,  6.4935e-01,\n",
       "         -3.0062e-01, -1.1635e-01,  6.9735e-01,  1.0930e+00,  1.1827e-01,\n",
       "          1.1171e+00,  8.9640e-01, -8.8429e-02, -5.6984e-01,  4.8745e-01,\n",
       "         -5.3238e-01,  7.1817e-01, -7.4240e-01, -3.7323e-01,  6.6385e-01,\n",
       "          4.6741e-01,  1.8085e-01,  1.3319e+00,  3.3838e-01, -7.2119e-01,\n",
       "          1.3193e+00, -3.1673e-01, -1.0429e-01,  1.1988e+00, -4.2214e-01,\n",
       "          1.4767e-01,  2.4867e+00, -7.2288e-01,  2.1142e+00, -1.8278e+00,\n",
       "         -1.2216e-01, -1.4321e-01,  7.7175e-01,  1.1279e+00,  5.5860e-01,\n",
       "          1.2219e+00, -1.6929e-01,  2.8094e-01,  5.1280e-01,  7.0349e-01,\n",
       "          1.8876e-01,  1.3438e-01,  8.3457e-01,  4.9367e-01,  1.4548e+00,\n",
       "          5.3679e-01, -1.5024e-01, -2.2819e-01,  5.1993e-01,  6.9244e-01,\n",
       "         -7.3904e-01,  1.1805e+00, -1.5081e-01,  1.6789e+00, -1.9912e-01,\n",
       "          1.6592e-01,  5.9325e-01,  5.6865e-01,  1.2124e+00,  1.2149e+00,\n",
       "          9.9757e-01,  2.0153e-01,  7.3687e-01, -2.5554e-01,  1.3881e+00,\n",
       "          3.5944e-01,  3.8903e-01,  1.5618e+00,  5.7702e-01,  9.7930e-01,\n",
       "          3.7765e-01,  5.0179e-01,  6.7992e-01,  1.6696e+00, -6.3226e-01,\n",
       "         -1.2316e+00, -9.7341e-01,  9.9299e-01,  7.5697e-01,  1.7843e+00,\n",
       "          1.8636e-01,  5.0664e-01,  1.4979e+00,  2.6310e-01,  3.3233e-03,\n",
       "          5.4174e-01,  1.0908e+00,  1.4840e+00,  1.1243e+00,  1.3563e-01,\n",
       "          4.7400e-02,  8.9369e-01,  7.9822e-01, -6.7791e-01,  5.8247e-01,\n",
       "         -5.6827e-01, -1.5892e-01, -1.2760e+00, -1.0647e+00,  1.1698e+00,\n",
       "          1.5102e+00,  3.7545e-01,  3.2802e-01,  1.3672e+00, -1.7150e-02,\n",
       "         -4.9402e-01,  9.8329e-01, -1.3865e-01,  1.7596e+00, -1.0503e+00,\n",
       "         -2.5766e-01,  4.0568e-01, -1.1479e+00,  1.6098e+00,  1.8743e-01,\n",
       "         -1.6835e+00, -1.0785e+00,  4.8291e-01,  7.4435e-01,  9.1218e-01,\n",
       "         -9.2938e-01,  5.4097e-01,  8.3998e-01,  1.2488e+00, -7.1992e-01,\n",
       "          9.2224e-01,  3.5769e-01, -9.6767e-01, -1.2219e+00,  1.6573e-01,\n",
       "          7.3191e-01,  1.5486e+00,  1.4205e+00,  1.4464e+00, -6.6386e-01,\n",
       "          1.5628e+00,  2.2716e-01,  5.4222e-01,  7.1276e-01,  5.9058e-01,\n",
       "          1.4990e+00,  8.5431e-01, -4.5560e-01,  5.8108e-01,  7.6943e-01,\n",
       "          1.2799e+00,  1.3861e+00,  1.8765e+00, -7.2351e-01, -4.0931e-01,\n",
       "          7.9456e-01, -5.2636e-01,  3.4167e-02, -3.5544e-01,  9.0894e-01,\n",
       "          3.7840e-01,  1.4105e+00,  1.0404e+00,  1.5571e-01, -3.3638e-01,\n",
       "          6.5103e-01,  2.6395e-01, -5.0292e-01,  1.2690e+00, -4.3481e-01,\n",
       "          8.3112e-01, -1.9810e+00,  1.1962e+00, -1.0032e+00, -2.4737e+00,\n",
       "          3.8809e-01,  1.5139e+00, -4.1717e-01, -1.1802e-01,  1.4487e+00,\n",
       "          9.2117e-01,  2.9344e-02,  1.0891e+00,  1.2159e+00,  1.1801e-01,\n",
       "          1.4494e-01, -3.3028e-01, -1.4488e-01, -9.6887e-01,  6.7076e-01,\n",
       "         -3.3636e-01,  1.1257e-01,  9.4836e-01,  1.3577e-01, -6.1367e-01,\n",
       "         -6.9731e-01,  9.7832e-01,  5.6462e-01,  1.8308e+00,  1.8865e+00,\n",
       "         -1.3805e+00, -3.4795e-01,  1.8922e+00,  8.3630e-01,  8.4292e-01,\n",
       "         -4.2921e-02, -4.0158e-01,  1.4542e+00, -9.1598e-01,  7.6037e-01,\n",
       "          1.4173e+00,  9.8469e-01,  8.9817e-01, -6.0070e-01, -2.0547e+00,\n",
       "         -5.2110e-01,  2.0973e-01,  3.9547e-01,  4.3534e-01,  1.8902e-01,\n",
       "         -3.5276e-01,  1.2426e+00, -7.2565e-01,  2.8910e-01, -2.8894e-01,\n",
       "         -1.0833e+00, -1.0037e+00, -6.0517e-01, -2.4207e-01,  1.6859e+00,\n",
       "         -3.7455e-01, -6.9552e-02,  4.0111e-01, -1.7471e+00, -9.6490e-03,\n",
       "         -6.2491e-01,  4.5664e-01,  1.6492e-01, -1.5596e-01, -5.6851e-02,\n",
       "         -2.7155e-01, -1.0455e+00, -7.9525e-02,  1.9119e-01, -5.5664e-01,\n",
       "         -7.7261e-01, -1.3460e+00,  6.1289e-01,  6.7101e-01, -3.7636e-01,\n",
       "         -5.6293e-02, -5.6477e-01, -5.0949e-01,  2.1733e-01,  5.9614e-01,\n",
       "         -2.4995e-01, -2.4786e-01, -6.4583e-01,  1.7526e-02, -9.8347e-01,\n",
       "          1.6769e-01,  4.9364e-01, -8.7968e-01, -9.9307e-01, -1.4905e+00,\n",
       "         -1.7912e-01,  4.7147e-01, -6.9225e-01,  7.9357e-01,  2.3569e-01,\n",
       "         -8.9829e-03,  9.1280e-01, -3.6767e-01, -5.7270e-01, -2.0593e+00,\n",
       "          9.9492e-01, -1.6227e+00,  4.2453e-01,  2.8480e-01, -7.4861e-01,\n",
       "         -6.4764e-01, -1.5699e-01,  5.8049e-01, -6.5716e-01, -9.0073e-01,\n",
       "         -1.5565e+00, -2.4525e+00,  1.3058e+00, -2.0432e-01, -9.2358e-01,\n",
       "         -3.1368e-01, -1.1876e+00, -8.0013e-01, -1.6451e+00, -7.6759e-01,\n",
       "         -2.7667e-01,  2.9954e-01, -3.8808e-01,  1.2583e+00,  1.0184e+00]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(data)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [p for p in model.parameters()]\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiation in autograd\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 81.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(9*a**2 == a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# add full path of nn-train\n",
    "path = \"c:\\\\somethingsomethingsomething\\\\nn-train\"\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_stuff.model_loader import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnected(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=100, out_features=10, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = load('fc2')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fc.2.weight'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = net.state_dict()\n",
    "layers = list(net.state_dict().keys())\n",
    "layers[1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[layers[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fc.0.weight', 'fc.2.weight']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc.0.weight',\n",
       "              tensor([[ 0.0085, -0.0057,  0.0177,  ...,  0.0009,  0.0067,  0.0109],\n",
       "                      [-0.0076, -0.0237, -0.0259,  ..., -0.0123,  0.0231, -0.0169],\n",
       "                      [ 0.0065, -0.0035,  0.0180,  ...,  0.0212, -0.0120, -0.0120],\n",
       "                      ...,\n",
       "                      [-0.0304, -0.0020, -0.0016,  ...,  0.0014,  0.0276, -0.0193],\n",
       "                      [-0.0342,  0.0078,  0.0146,  ...,  0.0251,  0.0339,  0.0330],\n",
       "                      [ 0.0151,  0.0116, -0.0292,  ...,  0.0264, -0.0156, -0.0001]])),\n",
       "             ('fc.2.weight',\n",
       "              tensor([[ 6.9402e-03, -7.2924e-02, -6.9672e-02, -7.9167e-02,  6.7816e-02,\n",
       "                       -1.0611e-03,  5.8461e-03, -1.6197e-02, -9.3281e-02,  2.8197e-03,\n",
       "                       -2.4915e-02, -2.5322e-02, -4.2666e-02, -1.4481e-02, -6.2404e-02,\n",
       "                       -9.3008e-03, -4.2019e-02,  3.9232e-02,  9.0389e-02, -9.0098e-02,\n",
       "                        7.6410e-02, -4.5344e-02,  1.4145e-02, -8.0475e-02, -7.7248e-02,\n",
       "                       -1.9128e-02, -5.7045e-02,  7.3930e-02,  2.9127e-02,  6.1713e-02,\n",
       "                        8.3235e-02,  9.9381e-02,  6.3387e-02, -9.6922e-02, -4.1576e-02,\n",
       "                       -6.4725e-02, -3.1131e-02, -1.0471e-02,  5.2646e-02, -8.8800e-02,\n",
       "                       -4.8405e-02,  4.3966e-02, -2.1380e-02,  9.4436e-02,  4.4363e-02,\n",
       "                       -6.7821e-02,  1.7697e-02, -9.4059e-02, -9.6054e-02, -1.1717e-02,\n",
       "                        3.1741e-02,  1.8102e-02, -8.9959e-02,  3.3471e-02, -4.2026e-02,\n",
       "                        8.9462e-02,  2.1152e-02,  1.7162e-02, -4.4363e-02, -5.0024e-02,\n",
       "                        5.6770e-03,  8.3397e-02, -2.4346e-02,  8.6875e-02,  7.8511e-02,\n",
       "                       -2.0521e-02,  2.4743e-02, -6.5278e-02,  6.0740e-02,  7.3186e-02,\n",
       "                       -5.2218e-02,  7.0214e-02,  5.0625e-02, -2.7506e-02, -1.6904e-02,\n",
       "                       -6.1659e-02, -7.1397e-02,  8.6524e-02,  4.5779e-02, -4.5560e-03,\n",
       "                        4.1829e-02, -3.6447e-02,  5.3450e-02, -9.5464e-02, -2.8137e-02,\n",
       "                        6.1450e-02,  5.3527e-02,  3.7629e-02,  7.9755e-02, -6.0519e-02,\n",
       "                       -5.0003e-03, -4.2653e-02, -9.5085e-03, -1.1639e-02, -2.9223e-02,\n",
       "                        7.7102e-02, -7.7158e-02, -1.1645e-02,  1.6152e-02,  9.6540e-02],\n",
       "                      [ 9.8211e-02, -7.8603e-03, -8.4503e-02,  4.3038e-02, -9.5869e-02,\n",
       "                       -4.5533e-02,  2.1878e-02, -8.5891e-02,  6.9835e-02, -4.0329e-02,\n",
       "                        3.3256e-02, -5.5021e-02, -5.6172e-02,  7.6002e-02, -7.7155e-02,\n",
       "                        3.7390e-03, -4.0997e-02,  5.2558e-02,  2.4049e-02,  2.6309e-02,\n",
       "                       -9.2359e-02,  5.9553e-02, -3.9466e-02, -1.8960e-02, -4.2734e-02,\n",
       "                       -8.3242e-02, -1.2234e-02,  6.5612e-02, -8.2281e-02, -3.6369e-02,\n",
       "                        7.3019e-02,  8.2166e-02, -1.8986e-02, -3.4627e-02,  4.2954e-02,\n",
       "                       -9.8300e-02,  4.8489e-02, -4.9004e-02,  1.7918e-02, -2.8685e-02,\n",
       "                        4.2229e-02,  6.8391e-02,  4.3677e-02, -5.1759e-03,  4.7787e-03,\n",
       "                       -1.8894e-02,  1.9744e-02,  9.6656e-02,  5.7974e-02, -2.1798e-02,\n",
       "                       -8.1025e-02, -7.1374e-02, -2.8718e-02, -5.1359e-02, -6.8825e-02,\n",
       "                       -6.8164e-03,  9.6943e-02, -2.8834e-02, -2.8088e-02,  4.3487e-02,\n",
       "                        3.0099e-02, -7.2487e-02, -4.7073e-02,  3.3931e-02, -4.5385e-02,\n",
       "                        9.7155e-02,  4.0117e-02, -2.6180e-02, -9.4177e-02,  7.2889e-04,\n",
       "                        1.5686e-02,  5.6070e-02, -5.4060e-02,  7.8331e-02,  3.0653e-02,\n",
       "                        4.9943e-02,  4.5343e-02,  9.9889e-02, -4.5659e-02,  9.6422e-02,\n",
       "                       -5.9987e-02,  5.6158e-02,  1.9416e-03,  3.7377e-02, -6.5582e-02,\n",
       "                       -5.0090e-02,  7.5712e-03, -2.6886e-02, -5.1393e-02, -2.2340e-02,\n",
       "                       -3.8444e-02, -9.0798e-02, -7.8709e-02, -9.7644e-02, -2.2425e-02,\n",
       "                       -8.2317e-02, -5.1275e-02, -8.0015e-02, -7.4754e-02,  3.6237e-02],\n",
       "                      [ 1.9246e-02, -2.7342e-02, -3.3545e-02,  6.0308e-02,  1.6752e-02,\n",
       "                       -2.0287e-02,  5.3930e-02,  3.9222e-02,  8.6968e-02, -7.7396e-02,\n",
       "                        6.5953e-02, -5.8632e-02,  3.8277e-02, -1.4444e-02, -3.9053e-02,\n",
       "                       -4.6721e-03, -6.8475e-02, -8.6163e-02, -6.9518e-02, -1.4783e-02,\n",
       "                        5.3997e-02, -4.8262e-02, -4.8289e-02,  1.1078e-02,  1.2244e-02,\n",
       "                       -2.6226e-02,  1.8374e-02,  1.0935e-02,  3.7690e-02, -7.6249e-02,\n",
       "                        9.4684e-02, -2.5595e-02, -3.7976e-02,  2.3992e-02,  6.2441e-02,\n",
       "                        5.3083e-02, -3.8625e-02, -4.1097e-02, -8.2221e-02, -4.9980e-02,\n",
       "                       -5.8310e-02, -1.9858e-02, -7.0547e-03,  8.2176e-02,  3.4634e-02,\n",
       "                       -6.1698e-02,  5.3249e-02, -3.2171e-03, -5.4004e-02, -8.1166e-02,\n",
       "                        7.7576e-02, -7.7170e-02, -4.2356e-02,  5.7212e-02, -2.0787e-02,\n",
       "                        1.5192e-02,  3.1307e-02,  8.8086e-02,  4.2147e-02,  2.0587e-02,\n",
       "                        5.3706e-02, -5.7132e-02, -8.3529e-02, -7.9793e-03, -9.5731e-02,\n",
       "                       -4.4188e-02,  5.6929e-03, -7.5986e-03, -2.4913e-02,  4.8498e-02,\n",
       "                        7.9728e-02, -4.4584e-02, -4.1150e-02, -5.8469e-02,  2.8444e-02,\n",
       "                        7.4722e-02,  6.2964e-02, -9.6664e-02,  7.6019e-02,  8.2327e-03,\n",
       "                        5.5063e-02,  4.4287e-02, -4.2031e-02, -3.0675e-02,  7.9873e-02,\n",
       "                        9.0739e-02, -3.5811e-02,  7.9648e-02, -4.1227e-02, -8.7062e-02,\n",
       "                       -6.1365e-02, -1.4122e-02, -8.8212e-02, -9.8713e-02,  5.1592e-02,\n",
       "                        6.4379e-02, -4.7934e-02,  7.1583e-02, -6.8511e-02, -2.1151e-02],\n",
       "                      [-8.6859e-02, -7.3469e-02,  2.4675e-02, -2.3438e-02, -7.4639e-02,\n",
       "                        8.3504e-02,  8.1420e-02,  8.3339e-03,  2.9609e-02,  3.2374e-02,\n",
       "                        6.4287e-04,  3.8164e-02,  1.6115e-03, -5.6411e-02,  8.3168e-02,\n",
       "                        6.7205e-02, -2.4499e-02,  8.6120e-02, -3.8986e-02,  5.0342e-02,\n",
       "                       -1.4861e-02,  3.6472e-02, -4.7255e-02,  4.1606e-02, -2.8769e-02,\n",
       "                       -3.2422e-02, -2.3665e-02,  4.6424e-02, -7.9359e-02, -2.7006e-02,\n",
       "                       -5.7234e-02, -6.5184e-02, -6.1851e-03, -1.9812e-02,  9.2476e-02,\n",
       "                        9.9188e-02, -8.7195e-02,  3.5844e-02, -6.1398e-02, -1.8034e-02,\n",
       "                        1.0364e-02, -2.7094e-02, -5.1018e-02,  4.0288e-03,  2.9969e-02,\n",
       "                        1.4319e-02,  6.1665e-03,  3.5749e-02,  5.1947e-02, -1.5314e-02,\n",
       "                       -6.3969e-02,  7.6785e-02,  2.7018e-02,  2.9508e-02,  9.9698e-02,\n",
       "                        8.7734e-02, -4.1603e-02, -7.6567e-02,  3.7432e-02,  9.5697e-02,\n",
       "                       -8.0688e-02,  9.9170e-02,  6.3603e-02, -6.1716e-02,  8.0241e-02,\n",
       "                       -1.9898e-02, -3.8303e-03, -7.2116e-02,  2.1136e-02, -2.6480e-02,\n",
       "                       -2.3476e-02, -4.2580e-02, -8.2799e-02,  2.1278e-02, -2.7218e-02,\n",
       "                        5.6976e-02, -9.3887e-02,  8.1116e-02, -3.7474e-02, -3.4682e-02,\n",
       "                       -8.7140e-02,  1.4774e-02,  4.8148e-02, -9.4808e-02, -7.2417e-02,\n",
       "                        9.5283e-02,  7.8902e-02,  5.9830e-02,  9.0977e-02, -3.7617e-02,\n",
       "                       -7.8251e-02, -7.7367e-02,  3.5135e-02,  6.5523e-02,  7.6728e-03,\n",
       "                       -2.7942e-02, -2.8401e-02,  7.3225e-02,  9.4054e-02, -7.0509e-02],\n",
       "                      [ 9.7590e-02, -8.0196e-03, -2.6323e-02,  4.5484e-02, -9.2668e-02,\n",
       "                       -9.5773e-02, -6.5676e-02,  1.8029e-02,  7.1738e-03, -3.1938e-02,\n",
       "                       -2.0944e-02,  4.7880e-02, -3.6681e-02,  6.7999e-02,  9.0155e-02,\n",
       "                        5.5599e-02, -2.8599e-02,  2.5627e-02, -2.1923e-02,  5.3531e-02,\n",
       "                       -7.8501e-02,  3.8248e-02,  7.8040e-03,  8.3445e-02, -7.2740e-02,\n",
       "                       -5.6466e-02, -6.1867e-02,  4.6334e-02, -5.2767e-02,  8.1409e-02,\n",
       "                       -2.9080e-02, -1.4112e-02, -8.4966e-02, -5.3111e-02, -2.2300e-02,\n",
       "                        6.6026e-02, -1.3860e-02, -8.4237e-02, -3.7495e-02, -5.3582e-02,\n",
       "                       -6.6839e-02, -7.8223e-02, -8.7218e-03, -4.8897e-03,  9.4536e-02,\n",
       "                        9.8416e-02, -4.9936e-02,  3.3021e-02,  7.2973e-02, -1.3771e-02,\n",
       "                       -5.0554e-02,  7.3261e-02,  5.4739e-02, -5.3918e-02,  5.8051e-02,\n",
       "                        8.8421e-02, -7.0982e-02, -8.8021e-02, -6.9547e-02,  3.5768e-02,\n",
       "                        9.1151e-02, -6.4393e-02, -5.1386e-02,  4.2643e-02,  3.5967e-02,\n",
       "                        3.2762e-02,  1.6002e-02,  6.9476e-02,  5.3350e-02, -4.4828e-02,\n",
       "                        8.8480e-02, -4.8393e-02,  9.5062e-02, -5.0105e-02, -3.2985e-02,\n",
       "                        1.3367e-02, -8.2797e-02,  2.8346e-02,  7.2549e-02,  3.2667e-02,\n",
       "                       -2.8604e-02,  4.1089e-02,  9.5452e-02,  6.5971e-02,  6.4850e-03,\n",
       "                        9.8001e-02, -8.7078e-02,  6.6330e-02, -6.2975e-02,  2.6734e-03,\n",
       "                       -2.1456e-02, -8.9337e-02, -1.5822e-02, -4.8953e-02, -9.6784e-02,\n",
       "                        9.0200e-02,  1.6998e-02, -7.4124e-02,  5.6608e-04,  6.6377e-02],\n",
       "                      [ 6.9490e-03,  3.8355e-02,  3.9589e-02,  5.5425e-02,  5.7465e-02,\n",
       "                       -2.7439e-02, -4.3931e-03, -4.3986e-02, -4.8560e-02, -5.6496e-02,\n",
       "                        7.8904e-02,  1.7561e-02,  5.9752e-02,  1.8322e-02, -2.0721e-02,\n",
       "                       -9.3135e-02,  5.3569e-02, -1.8865e-02, -7.5402e-02, -4.4391e-02,\n",
       "                        4.8603e-02,  1.4850e-02, -8.5470e-03,  5.1178e-02, -9.5238e-02,\n",
       "                       -2.4385e-02, -2.1791e-02,  5.4685e-02,  6.2330e-02, -2.1483e-02,\n",
       "                       -5.7498e-04,  5.3853e-02, -6.3437e-02, -6.6441e-03, -7.7505e-02,\n",
       "                        9.3487e-02,  8.3609e-02, -7.5706e-02, -5.6365e-02,  5.9879e-02,\n",
       "                        2.0557e-02,  8.6059e-02,  1.1117e-02, -1.4254e-02, -3.4669e-02,\n",
       "                       -6.9863e-02,  3.9619e-02, -2.5953e-02,  9.9592e-02, -3.5574e-02,\n",
       "                       -8.8297e-03, -6.8250e-02, -2.3092e-02, -1.9532e-02,  4.3083e-02,\n",
       "                        1.9061e-02,  8.9719e-02, -8.5574e-02,  4.9991e-02,  7.8632e-02,\n",
       "                       -8.4097e-02,  1.6796e-02, -3.0393e-02, -1.2828e-02,  8.3598e-03,\n",
       "                       -7.1632e-02,  7.5606e-02,  1.6333e-02,  9.8598e-02,  1.4444e-02,\n",
       "                       -2.2179e-02, -4.0165e-02, -1.4409e-02, -8.0462e-02, -6.8898e-02,\n",
       "                        1.0168e-02, -2.0313e-04, -8.5592e-02, -2.2607e-02, -2.3598e-02,\n",
       "                       -9.0691e-02, -7.9179e-02, -4.3586e-02, -9.3552e-02,  7.5362e-02,\n",
       "                       -1.1850e-02, -8.1754e-03, -2.0252e-03, -2.6059e-02, -2.6358e-02,\n",
       "                       -6.4694e-02,  2.8266e-02,  3.9543e-02,  4.9615e-02,  2.3847e-02,\n",
       "                       -4.1258e-03, -5.8118e-02,  7.1941e-03, -8.2410e-02,  2.1373e-02],\n",
       "                      [-9.2650e-02, -4.0642e-03, -1.4626e-03,  1.5699e-02,  2.3029e-02,\n",
       "                       -2.8248e-02,  8.6093e-02, -1.2259e-02, -9.5030e-02, -3.1717e-02,\n",
       "                       -3.3619e-03, -1.9446e-02,  8.4021e-02,  5.3896e-02,  7.0070e-02,\n",
       "                       -5.5355e-03,  3.5706e-02,  4.6770e-02, -7.3955e-02, -3.2590e-02,\n",
       "                        6.1004e-02,  2.5599e-02,  6.5247e-02,  7.0625e-02, -7.4432e-02,\n",
       "                        2.3261e-02,  5.1651e-02, -8.0368e-02, -3.9394e-02,  6.7884e-02,\n",
       "                        9.8070e-02, -4.7883e-03,  4.6494e-02, -9.1658e-02,  2.9716e-02,\n",
       "                       -8.7371e-02, -6.3387e-03, -6.7016e-02,  4.2497e-02,  6.2283e-02,\n",
       "                       -9.8050e-02,  4.0923e-02, -6.2728e-02,  2.2629e-02,  4.8940e-02,\n",
       "                       -4.1573e-02, -8.3969e-02, -8.4122e-02,  2.3411e-02, -4.4703e-02,\n",
       "                       -3.6766e-02, -7.2182e-02, -5.8660e-02, -2.2898e-02, -7.4011e-02,\n",
       "                       -3.7999e-02,  9.9232e-02,  6.4518e-02, -8.8302e-02, -9.1300e-02,\n",
       "                       -8.9950e-02,  2.9088e-02,  6.1577e-02, -3.8332e-02,  5.8796e-02,\n",
       "                        5.9601e-02, -6.4299e-03, -4.0817e-03,  7.2192e-02, -5.7170e-02,\n",
       "                        1.1599e-02,  3.3700e-02, -1.8058e-02,  3.2585e-02,  6.5574e-02,\n",
       "                        2.5565e-02,  2.9685e-02, -2.4524e-02, -1.5932e-02, -4.5795e-02,\n",
       "                        5.0075e-02,  1.3914e-02, -6.2488e-02,  7.4011e-02, -1.7560e-02,\n",
       "                       -2.9268e-02,  8.6935e-02,  7.5875e-02, -9.7428e-02,  1.5070e-02,\n",
       "                        8.9419e-02,  9.7428e-02,  7.8431e-02, -6.8535e-02,  1.3587e-02,\n",
       "                        4.6405e-02, -2.1106e-02, -3.7692e-02, -4.7363e-02, -2.1687e-02],\n",
       "                      [-5.5322e-02,  3.0016e-02,  3.9810e-02,  5.3584e-02,  1.9668e-02,\n",
       "                        2.5411e-02,  9.6229e-02, -3.2113e-02, -9.9021e-02, -4.0984e-02,\n",
       "                        8.5199e-03, -5.7389e-02, -1.2626e-02,  3.2811e-02, -7.7053e-02,\n",
       "                       -7.4263e-02,  2.7897e-02,  4.9471e-02, -2.7567e-03,  4.0719e-03,\n",
       "                        8.7510e-02, -1.7450e-02, -9.1947e-02, -1.7841e-02,  3.0565e-02,\n",
       "                       -1.9351e-02, -7.2325e-02, -1.9008e-02,  6.8313e-02, -4.7148e-02,\n",
       "                        5.6539e-02,  7.7758e-02, -8.4255e-02,  9.6523e-02, -6.8790e-02,\n",
       "                       -3.8346e-02,  9.5970e-02,  1.5392e-02,  9.7480e-02,  2.7742e-02,\n",
       "                        5.4199e-04,  4.4489e-02,  3.9558e-02,  7.8014e-03, -7.6858e-02,\n",
       "                       -1.2585e-02,  6.3566e-02,  7.1071e-02, -8.8259e-02, -6.0089e-02,\n",
       "                       -6.1264e-02,  5.2125e-03, -8.9673e-02, -3.3692e-02,  2.8739e-03,\n",
       "                       -3.9533e-02,  6.1977e-02, -2.3544e-02, -4.3215e-02, -4.5116e-03,\n",
       "                       -7.0625e-02,  6.5183e-02, -6.0033e-02,  2.2877e-02, -4.1015e-03,\n",
       "                        4.2758e-02, -4.6249e-02, -6.6030e-02,  2.7129e-02,  9.0558e-02,\n",
       "                        8.8050e-02,  2.7996e-02, -9.7603e-02,  3.9135e-02,  1.5256e-02,\n",
       "                        5.5386e-02, -5.1021e-02,  5.0178e-02,  4.4318e-02,  9.1030e-02,\n",
       "                        9.6428e-02,  4.5463e-02,  6.7331e-02,  8.5634e-02, -5.8746e-02,\n",
       "                       -2.8673e-03,  4.5222e-02, -3.6509e-02,  1.5973e-02, -6.4826e-02,\n",
       "                        6.3498e-02,  4.9492e-04,  8.2924e-02, -8.3723e-02,  1.7262e-02,\n",
       "                        6.4001e-02,  7.7808e-02, -3.7934e-04,  8.1772e-02, -8.7659e-02],\n",
       "                      [-2.7592e-02, -1.4208e-02, -3.0583e-02, -6.6034e-02, -2.5673e-02,\n",
       "                       -4.0935e-02,  7.1625e-02,  8.5175e-02, -8.8167e-02, -8.6496e-02,\n",
       "                       -4.2542e-02,  9.0135e-03,  4.5229e-02, -1.8144e-02,  3.3167e-02,\n",
       "                       -6.9967e-02, -3.4522e-02, -4.5987e-02, -4.8135e-02, -9.8948e-02,\n",
       "                        2.6064e-02, -9.8290e-04, -3.9782e-02, -3.6328e-02,  1.5677e-02,\n",
       "                       -1.1268e-02,  2.8613e-02,  8.1641e-03,  8.4090e-02,  5.3579e-02,\n",
       "                        8.4102e-02,  4.9303e-02,  7.9893e-02,  2.3651e-02, -8.5569e-02,\n",
       "                        3.9661e-02, -2.3971e-02,  5.1820e-02, -2.0003e-02, -9.6669e-02,\n",
       "                        5.4054e-02,  8.2716e-02,  2.4578e-02, -9.2624e-02,  9.9471e-02,\n",
       "                        4.5802e-02, -3.5393e-02,  3.8841e-03, -1.5576e-02, -6.4118e-02,\n",
       "                       -8.5380e-02,  2.5790e-02, -1.5464e-02, -9.4859e-02, -3.7558e-02,\n",
       "                       -4.7536e-02, -8.1873e-02, -7.4042e-02,  6.2525e-02, -5.8457e-02,\n",
       "                        5.0767e-02, -6.1454e-02,  2.0593e-02, -7.0348e-02,  5.7375e-02,\n",
       "                       -6.5889e-02, -3.4147e-02,  9.3301e-02, -5.0118e-02,  3.7217e-02,\n",
       "                       -3.7819e-02, -3.0902e-02,  4.9406e-02, -3.7446e-03, -3.4703e-02,\n",
       "                       -1.0499e-03, -1.7611e-02, -5.2838e-02, -4.3717e-02, -3.9080e-02,\n",
       "                        9.3213e-02, -8.7531e-02,  9.2546e-02, -4.3683e-02, -6.5898e-02,\n",
       "                        3.7213e-02,  3.4499e-02, -7.1663e-02,  7.3199e-02, -2.5806e-02,\n",
       "                       -1.3433e-02,  1.7667e-02,  7.1223e-02,  7.6966e-02,  1.3051e-02,\n",
       "                       -2.9709e-02, -7.0880e-02,  5.1557e-02,  7.9131e-02,  4.5135e-02],\n",
       "                      [-9.8766e-02,  7.9706e-02,  1.2813e-03,  9.7332e-03, -1.3347e-02,\n",
       "                        9.3533e-02,  5.5801e-02,  1.2019e-02,  1.5882e-02, -2.6575e-02,\n",
       "                        7.3657e-02,  4.5761e-02, -3.9943e-02,  7.9193e-02, -3.1596e-02,\n",
       "                       -2.7232e-02, -1.0199e-02,  6.3524e-02,  3.1402e-02, -8.8269e-02,\n",
       "                       -3.2812e-02, -5.9535e-02, -8.3748e-02,  2.0359e-02,  7.1558e-02,\n",
       "                       -3.9438e-02,  5.1965e-03,  5.8182e-02,  6.2719e-05,  4.3695e-02,\n",
       "                       -7.5285e-02,  6.3379e-02, -2.9386e-02, -5.5252e-02, -3.9400e-02,\n",
       "                        6.4878e-02,  9.5689e-02,  7.9862e-03,  8.4891e-02, -7.5428e-02,\n",
       "                       -6.3325e-02,  1.9562e-02,  5.9961e-02,  6.1375e-02, -3.6090e-03,\n",
       "                       -7.3050e-02,  4.8612e-02, -3.1432e-03, -4.6242e-02,  6.2475e-02,\n",
       "                        4.6721e-02, -6.7706e-02, -4.9518e-02,  3.7732e-02,  3.7266e-02,\n",
       "                        4.9596e-02, -6.3087e-02,  8.5799e-03, -9.4107e-02, -2.8891e-02,\n",
       "                        7.5512e-02, -5.9365e-02,  8.8300e-02,  7.0443e-02,  1.1879e-02,\n",
       "                        1.7552e-02, -6.8420e-02,  2.4246e-02, -9.9534e-02,  6.5105e-02,\n",
       "                        8.9706e-02, -2.4092e-02,  9.5434e-02,  2.5834e-03, -8.6204e-02,\n",
       "                       -5.8739e-02, -2.9400e-02,  5.5877e-02, -6.7035e-02,  8.7582e-03,\n",
       "                        2.9319e-02,  3.4367e-02, -1.0955e-03, -3.5389e-02,  2.9555e-02,\n",
       "                        6.9362e-02,  7.2540e-02,  7.0656e-02,  6.0624e-02,  3.0770e-02,\n",
       "                        8.6858e-03, -5.9855e-02, -4.8132e-02, -3.9129e-02,  5.9229e-02,\n",
       "                        9.7029e-02, -9.3360e-02,  7.7829e-03,  3.1618e-02, -5.1606e-02]]))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0740, -0.5930, -0.2624,  0.0965, -0.2836,  0.0191, -0.0337, -0.2314,\n",
       "         -0.1131,  0.4927]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.ones(1,784)\n",
    "net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "2. Deep Learning, Goodfellow, I., Bengio, Y., Courville A.\n",
    "\n",
    "3. Programming PyTorch for Deep Learning, Pointer, I."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
