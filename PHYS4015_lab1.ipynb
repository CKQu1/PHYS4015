{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS4015 Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import scipy.io as sio\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Data objects as torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "type(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Random number generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.3433, 0.1685],\n",
      "        [0.2095, 0.2066]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes\n",
    "shape = (2,3,4)\n",
    "rand_tensor = torch.rand(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0753, 0.6563, 0.4151, 0.2865],\n",
      "         [0.6873, 0.6049, 0.2861, 0.1231],\n",
      "         [0.7049, 0.3586, 0.3321, 0.1129]],\n",
      "\n",
      "        [[0.0841, 0.3433, 0.7291, 0.7803],\n",
      "         [0.9026, 0.0331, 0.1987, 0.9326],\n",
      "         [0.1192, 0.4779, 0.9468, 0.8188]]])\n"
     ]
    }
   ],
   "source": [
    "print(rand_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor operations\n",
    "tensor = torch.ones(4,4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim = 1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4., 4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "a = torch.ones((1,4))\n",
    "b = torch.ones((4,4))\n",
    "c = torch.mm(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "a = torch.ones((4,))\n",
    "b = torch.ones((4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 4., 4., 4.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a,b)\n",
    "# a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1,3,64,64)\n",
    "labels = torch.rand(1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5030, -0.2340, -0.5639, -1.5148, -0.6230,  0.0814, -0.5804,  0.2441,\n",
       "          0.2233, -0.8696, -1.0280, -1.0076, -0.3329, -0.8329, -1.3373, -0.6137,\n",
       "         -0.8142, -0.4510, -0.8234, -0.4968, -1.5436, -0.9320, -1.3844, -0.0749,\n",
       "         -1.1843, -1.4563, -0.8158, -0.9225, -1.0206, -0.2020, -0.8342, -0.6204,\n",
       "         -0.4199, -0.6239, -0.4610, -0.5273,  0.3549, -0.9955, -0.4208,  0.2968,\n",
       "         -0.6830, -0.8550, -1.1449, -0.1389, -0.5927, -0.6046, -0.8142, -0.4349,\n",
       "         -1.0055, -1.3128, -0.7527,  0.4317, -0.4421, -0.7455, -0.2577, -1.1418,\n",
       "         -0.4600, -1.5786, -0.7691, -0.4451,  0.6382,  0.0114, -0.1962,  0.1521,\n",
       "         -0.8537, -0.3551, -0.4125, -0.4927, -0.9104, -0.9867, -1.5906,  0.2516,\n",
       "         -1.6692, -0.3279, -1.2900, -1.5682, -0.0254, -0.7399,  0.2676, -0.1971,\n",
       "         -0.9884, -1.6603, -0.1184, -0.9318, -0.7200, -0.1097,  0.0592,  0.3678,\n",
       "         -0.2632, -0.8555, -1.3695, -1.3130, -2.1402, -0.5195, -0.0475, -2.5156,\n",
       "         -0.5738, -0.5164, -1.5074, -0.1217, -1.5111, -1.0201, -0.8869, -0.3402,\n",
       "         -0.2276, -0.4796, -0.2506, -1.2576, -0.8539, -1.5645, -1.2392, -0.4333,\n",
       "          1.2863,  0.2442,  0.1898, -1.1483, -0.4489,  0.0030,  0.6183, -0.4372,\n",
       "         -0.7069,  0.1292,  0.2632,  0.1763,  0.8941, -0.0707,  0.2218, -1.8050,\n",
       "         -1.7412, -1.3131, -1.2635, -1.7813, -1.1355, -1.6086, -0.9735, -1.6188,\n",
       "         -1.2564, -1.1507, -1.3954, -1.9759, -1.6870, -1.8782, -2.3223, -1.9520,\n",
       "         -0.8039, -0.4152, -0.9478, -1.8461, -1.0927, -1.1204,  0.3469,  1.6257,\n",
       "         -1.0668, -0.3234, -0.0531,  0.0906, -0.2208, -0.3240,  0.0554,  0.1205,\n",
       "          0.5397,  0.6557,  0.3512,  0.4225,  0.1385, -0.4794, -0.2835, -0.4223,\n",
       "          0.2981, -0.3507, -0.2417,  0.7829,  0.3243,  0.3381,  0.2029, -0.8325,\n",
       "          0.1214, -0.1346,  0.5189,  0.5589,  0.5429,  0.1838,  0.3280, -0.1307,\n",
       "          0.5270,  0.6871,  0.5969,  0.2320,  0.0575,  0.5966, -0.4720,  0.3109,\n",
       "          0.1220,  0.6266, -0.5305,  0.6366,  0.1413,  0.0360,  0.3210,  0.5387,\n",
       "         -0.1015,  0.0683,  0.2349,  0.5903,  0.0391,  0.0633, -0.2229,  0.4511,\n",
       "          1.0374,  0.3004, -0.3892,  0.2341,  0.0833, -0.2057, -0.1463,  0.1955,\n",
       "         -0.1688,  0.1696, -0.4320,  0.7725,  0.1678, -0.1080,  0.1657,  0.6912,\n",
       "          0.2456,  0.1785, -0.2352,  0.8320, -0.5668, -0.0905, -0.1782,  0.4650,\n",
       "          0.3758, -0.1825,  0.5752,  0.5695,  0.3123,  0.3229,  0.6167, -0.1219,\n",
       "          0.3195, -0.1969,  0.3049,  0.1103, -0.3960,  0.3606,  0.1547, -0.1531,\n",
       "          0.8252,  0.2445,  0.7059,  0.4976, -1.0485,  0.6718,  0.7921, -0.8815,\n",
       "          0.3879,  0.2566,  0.1109,  0.2685, -0.1209, -0.5354, -0.3357,  0.2258,\n",
       "          0.6657,  0.6281,  0.2721,  0.4674, -0.2821, -0.4079, -0.6932, -1.0852,\n",
       "         -0.8591,  0.7178, -1.0481, -1.3765, -1.2528, -0.8587, -1.4905, -0.2139,\n",
       "         -0.2104,  1.0366,  0.7892,  0.3208,  0.2707,  0.9391,  0.0066, -0.2445,\n",
       "         -0.6638, -1.4600, -1.1169, -1.2157, -0.4625, -1.1190, -0.9504, -1.0514,\n",
       "         -0.8161, -1.2641, -0.2569,  0.0892, -2.2764, -0.8979, -0.6774, -0.5958,\n",
       "         -1.3792, -1.0926,  0.0575, -1.1180, -1.9016, -0.8652,  0.3075, -0.7305,\n",
       "         -0.7747,  0.0526,  0.7618, -0.6147, -1.1739, -1.7489, -1.2012, -1.0034,\n",
       "         -1.7082, -1.2236, -1.3160, -1.6056, -1.3953, -1.3844, -1.1701, -0.0923,\n",
       "          0.0571, -0.5791, -0.2097, -0.5831, -0.1788,  0.3106, -0.5315, -0.7135,\n",
       "         -1.7405, -0.1215,  0.5016, -1.0975, -0.4584,  0.7159, -0.4672, -1.3602,\n",
       "         -0.8677,  0.6533, -0.7772, -1.6789, -0.2442, -1.3291, -1.1923, -2.2916,\n",
       "         -1.3322, -0.8601, -0.5089,  0.2618,  1.0971, -0.1322,  0.5162,  0.3671,\n",
       "         -0.3022,  0.3768,  0.1356,  0.3920, -0.1141, -0.4859, -1.1681, -0.5096,\n",
       "         -0.8902, -0.7702, -0.5511, -0.0212, -0.5708,  0.0147, -0.0853, -0.7491,\n",
       "         -0.9213,  0.2996, -0.2277, -0.6393,  0.3389, -0.3386, -0.1281, -0.5392,\n",
       "         -0.5268, -0.4854, -1.0776, -0.9069, -0.7233, -0.2496,  0.4071,  0.3226,\n",
       "         -1.2126, -1.5832, -0.4534,  0.4354, -1.1269, -0.4616,  0.4044,  0.4811,\n",
       "         -0.6874,  0.9964,  0.3050, -2.1044, -2.2018, -0.5496, -0.2510, -0.1682,\n",
       "         -0.4272,  1.2981, -0.4732,  0.2875,  1.9544,  0.5384,  0.6783,  0.9270,\n",
       "         -0.5347,  0.3419,  0.2148,  1.2243,  1.0592,  1.2358, -0.3055,  0.5466,\n",
       "          0.6735, -1.0104,  0.2430,  1.2245,  1.6835,  0.4955, -0.5933, -0.1589,\n",
       "          0.4815,  0.7188,  0.6220,  1.1367, -0.4450, -0.1484,  0.6028,  0.7467,\n",
       "          0.8766,  0.6204,  0.4048, -0.3947, -0.3199,  0.2449,  0.5982,  1.4623,\n",
       "          0.9881, -0.6594, -0.5196,  0.6628,  0.8246, -0.2647, -0.2701,  0.6988,\n",
       "          1.7169,  1.0051,  0.2024,  1.0805, -0.8552,  0.7877,  1.4290,  2.6986,\n",
       "          0.9859, -0.1544, -1.2700, -0.2013,  0.0382,  1.7064,  1.2930,  0.4268,\n",
       "         -0.0152,  1.1619, -0.0396,  0.0576,  0.1280,  0.5875,  0.6801,  0.3956,\n",
       "          0.2604,  0.1770,  0.2504, -0.8478, -1.4551, -0.1244, -0.1634,  1.0715,\n",
       "          1.7167,  1.4780,  0.8464,  0.7636,  0.5972, -1.1235,  1.5629, -0.9215,\n",
       "         -0.1122, -0.2460, -0.0695,  1.5020, -1.7344,  0.9260,  0.9977,  0.4592,\n",
       "          1.0699,  1.2370,  1.1977,  0.8740,  0.3916,  0.0765, -1.1665, -0.8305,\n",
       "          0.9273,  0.4415,  0.9172,  2.0900,  0.3034, -0.2989,  1.0250,  0.3645,\n",
       "         -1.0943,  0.3174,  0.8107,  1.5093,  0.4993, -0.8545, -0.4176, -0.5764,\n",
       "          0.3801, -0.0114,  1.0849,  0.4294,  0.0730, -0.9792,  0.7033, -0.4714,\n",
       "         -0.5946, -0.5778,  0.5105,  1.3371, -1.2324,  1.8036,  1.2900,  0.6404,\n",
       "          0.9771,  1.1924,  0.7584, -1.9880, -1.6572,  0.1369, -0.4595,  0.3266,\n",
       "          0.7640, -0.2180, -1.7475, -0.7860,  0.6846,  0.6263,  1.3702,  0.9722,\n",
       "         -0.2113, -0.5118,  0.9829, -0.3504, -1.4012, -0.6747,  0.3713,  1.4168,\n",
       "          0.5433, -0.6308,  1.1359,  0.2031,  0.8359, -1.1339,  0.3955, -0.5987,\n",
       "         -0.8365,  1.3038,  0.3131,  0.1600,  0.1798, -0.1207,  0.8432,  0.5610,\n",
       "          1.0813,  1.0537, -0.7184,  1.5302,  1.2141,  1.3102, -0.5395,  0.5241,\n",
       "         -0.4627,  0.9484,  0.4440, -1.0682,  1.0413, -0.2068, -1.0270,  0.6392,\n",
       "          2.1985, -0.0282, -0.2097, -0.7433,  0.6722, -0.0108,  1.3175, -0.0818,\n",
       "          0.9449, -0.4236,  1.3092,  0.5055, -0.6495,  0.5845,  0.4670,  0.1444,\n",
       "          1.2367,  0.2901,  2.1993,  1.2534,  0.8508,  0.4503,  0.2395,  0.7353,\n",
       "          0.2705, -1.2025,  1.3256, -0.7031, -1.0041,  0.2255, -0.0470,  1.0881,\n",
       "          0.6451,  1.2403, -0.4242,  0.4871,  1.4366,  0.9439,  0.5643,  0.2330,\n",
       "         -1.3015,  1.2982, -0.1319,  1.6081,  0.8150, -0.6615,  0.6890,  0.4391,\n",
       "         -0.3798, -1.3692,  1.0687,  0.2228,  0.5259,  1.1439, -0.3049,  0.5941,\n",
       "         -0.1312,  0.0760,  0.1063,  0.6307, -0.2783, -1.4813, -0.1976, -0.5922,\n",
       "          0.6752,  0.1728,  1.0475,  0.5716, -0.3476, -0.6190,  0.3807, -0.4202,\n",
       "         -0.1186,  0.5525,  1.4017, -0.9282,  1.4714,  0.8840,  0.7870,  0.2535,\n",
       "          0.9635,  0.3715, -0.1328,  0.6988,  0.9865, -1.3128,  0.1222, -0.8970,\n",
       "         -0.2871, -0.5472, -0.7758,  0.8761,  0.8178,  0.4837, -1.0831,  1.0802,\n",
       "          1.6912, -0.1886, -0.2131,  0.6931,  1.7896, -0.4223, -0.1641,  0.3245,\n",
       "          0.7001, -0.6106, -0.3465,  0.3262,  1.2631,  0.1110,  0.9498,  1.0801,\n",
       "          0.3555, -0.5424,  0.5952, -0.0243,  0.9430, -0.8643, -0.2107,  0.6609,\n",
       "          0.6099,  0.1228,  1.4156,  0.4265, -0.6381,  1.4104, -0.8806, -0.1509,\n",
       "          1.5209, -0.3122,  0.0096,  2.2318, -0.9846,  1.8869, -1.8795,  0.2055,\n",
       "         -0.0875,  0.7889,  1.2552,  0.5285,  1.1093,  0.2531,  0.1777,  0.5159,\n",
       "          0.6957,  0.1102, -0.0476,  0.8778,  0.7597,  1.5842,  0.3666, -0.3917,\n",
       "          0.0471,  0.9629,  0.7946, -0.3305,  0.9066, -0.1559,  1.4779, -0.0151,\n",
       "          0.1443,  0.8262,  0.5299,  0.4907,  1.2489,  0.9139,  0.5073,  0.5303,\n",
       "         -0.0118,  1.4187,  0.5357,  0.7058,  1.8120,  0.7283,  0.8468,  0.5735,\n",
       "          0.4943,  0.9205,  1.3483, -0.7977, -0.6689, -0.9613,  1.0015,  0.6846,\n",
       "          1.3241,  0.3950,  0.4322,  1.6002,  0.4007,  0.1260,  0.7534,  1.0695,\n",
       "          1.5300,  0.8056,  0.3849, -0.0182,  1.1172,  0.8797, -0.6475,  0.2782,\n",
       "         -0.6775, -0.0368, -1.0882, -1.2853,  0.9861,  1.2831, -0.0185,  0.2255,\n",
       "          1.6680,  0.1816, -0.3834,  1.3578, -0.3540,  1.9364, -1.1802, -0.1442,\n",
       "          0.3139, -1.2856,  1.9576,  0.4715, -1.5760, -1.0139,  0.3824,  0.7763,\n",
       "          0.9583, -0.9264,  0.4989,  1.2495,  1.7820, -0.8653,  1.1335,  0.2171,\n",
       "         -0.4026, -1.2183,  0.0276,  0.9830,  1.8953,  1.8961,  1.2964, -0.6826,\n",
       "          1.2281,  0.7822,  0.3358,  0.4733,  0.8040,  1.8225,  0.9530, -0.4018,\n",
       "          0.5171,  1.1288,  1.3430,  1.2328,  1.7259, -0.3609, -0.0554,  1.0387,\n",
       "         -0.6564,  0.1818, -0.0764,  1.1505,  0.2079,  1.5654,  0.8972, -0.1243,\n",
       "         -0.7891,  0.5634,  0.0495, -0.4954,  1.8528, -0.3266,  0.8697, -1.4010,\n",
       "          0.8647, -0.8202, -2.5108,  0.4253,  1.4856, -0.2759, -0.1114,  1.9935,\n",
       "          1.2418, -0.1802,  0.7826,  1.3073,  0.4278,  0.5035, -0.3061,  0.1083,\n",
       "         -0.9850,  0.4420, -0.3919,  0.1932,  1.0614,  0.2044, -0.6320, -0.9026,\n",
       "          1.3441,  0.7753,  2.1334,  2.0083, -0.7693, -0.1522,  1.9865,  0.9861,\n",
       "          0.9440,  0.3830, -0.4038,  1.0220, -1.2646,  1.1915,  1.3839,  1.3158,\n",
       "          0.7485, -0.7241, -1.6904, -0.3674,  0.1031,  0.3408,  0.4452,  0.4511,\n",
       "         -0.1164,  1.2097, -0.5569,  0.7267, -0.4512, -0.7002, -0.8822, -0.4077,\n",
       "          0.0326,  1.5794, -0.0339,  0.0113,  0.6080, -1.4175,  0.0396, -0.7156,\n",
       "          0.3698,  0.3774,  0.1770, -0.2386, -0.3172, -0.7523,  0.0180, -0.0869,\n",
       "         -0.4801, -0.8010, -1.2243,  0.8018,  0.4845, -0.4345,  0.2198, -0.6148,\n",
       "         -0.7046, -0.0642,  0.5160, -0.5599, -0.3266, -0.6696, -0.1219, -1.1120,\n",
       "          0.3922,  0.1283, -0.4604, -0.6655, -1.1034, -0.0567,  0.3749, -0.5399,\n",
       "          1.0889,  0.3111, -0.3627,  1.1315, -0.5820, -0.3169, -1.8733,  0.9683,\n",
       "         -1.6071,  0.3244,  0.2071, -0.8113, -0.1707, -0.1492,  0.3735, -0.6294,\n",
       "         -1.0168, -1.0897, -2.3243,  1.3409, -0.1180, -0.7612, -0.0173, -1.1583,\n",
       "         -0.6749, -1.9272, -0.6992, -0.1284,  0.4678, -0.3130,  1.2346,  1.0644]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output of the network corredponding to the input\n",
    "prediction = model(data)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute gradients or differentiate, some sort of computation graph must be contructed: https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select an optimizer: https://pytorch.org/docs/stable/optim.html\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [p for p in model.parameters()]\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiation in autograd\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 81.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(9*a**2 == a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you specify `torch.no_grad()`, the gradients are no longer able to be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23820\\1821728173.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mexternal_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexternal_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    Q = 3*a**3 - b**2\n",
    "\n",
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from path_names import nn_path  # create your own DNN\n",
    "# add full path of nn-train\n",
    "#path = \"c:\\\\somethingsomethingsomething\\\\nn-train\"\n",
    "path = nn_path\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_stuff.model_loader import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnected(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=100, out_features=10, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = load('fc2')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['fc.0.weight', 'fc.2.weight'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fc.2.weight'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = net.state_dict()\n",
    "layers = list(net.state_dict().keys())\n",
    "layers[1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[layers[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fc.0.weight', 'fc.2.weight']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc.0.weight',\n",
       "              tensor([[ 0.0092, -0.0083,  0.0165,  ...,  0.0259,  0.0095, -0.0355],\n",
       "                      [ 0.0050, -0.0188,  0.0099,  ...,  0.0250, -0.0327, -0.0160],\n",
       "                      [-0.0102,  0.0046,  0.0077,  ..., -0.0342, -0.0055,  0.0088],\n",
       "                      ...,\n",
       "                      [-0.0205, -0.0198, -0.0065,  ...,  0.0148,  0.0088, -0.0151],\n",
       "                      [-0.0338, -0.0051,  0.0066,  ...,  0.0207,  0.0257, -0.0117],\n",
       "                      [-0.0011,  0.0324,  0.0229,  ..., -0.0104, -0.0246, -0.0247]])),\n",
       "             ('fc.2.weight',\n",
       "              tensor([[-0.0839,  0.0400, -0.0552, -0.0988, -0.0622, -0.0860, -0.0852,  0.0465,\n",
       "                       -0.0417,  0.0708, -0.0899, -0.0161, -0.0810,  0.0362,  0.0525,  0.0594,\n",
       "                       -0.0542,  0.0795,  0.0175, -0.0974, -0.0679,  0.0938, -0.0638, -0.0305,\n",
       "                        0.0792,  0.0588, -0.0396, -0.0280, -0.0620, -0.0228,  0.0621,  0.0878,\n",
       "                       -0.0310, -0.0464,  0.0173, -0.0101,  0.0494,  0.0978, -0.0541,  0.0819,\n",
       "                        0.0366, -0.0377, -0.0050,  0.0878,  0.0743, -0.0884,  0.0171, -0.0358,\n",
       "                       -0.0691, -0.0838,  0.0942,  0.0722, -0.0699, -0.0312, -0.0485, -0.0504,\n",
       "                        0.0481,  0.0985,  0.0446, -0.0388,  0.0659, -0.0409, -0.0330,  0.0238,\n",
       "                       -0.0915,  0.0900,  0.0694, -0.0555,  0.0678,  0.0920,  0.0655,  0.0462,\n",
       "                       -0.0284,  0.0898,  0.0263,  0.0394,  0.0946, -0.0741,  0.0782,  0.0564,\n",
       "                        0.0274, -0.0722,  0.0326,  0.0075, -0.0357, -0.0603,  0.0399, -0.0812,\n",
       "                       -0.0466,  0.0851, -0.0421,  0.0801, -0.0745,  0.0551,  0.0459, -0.0805,\n",
       "                       -0.0953, -0.0995, -0.0752,  0.0937],\n",
       "                      [-0.0369, -0.0133, -0.0844,  0.0746, -0.0869,  0.0198, -0.0756, -0.0003,\n",
       "                        0.0590,  0.0474, -0.0127, -0.0753,  0.0498, -0.0679,  0.0535,  0.0339,\n",
       "                        0.0929, -0.0987,  0.0962, -0.0339,  0.0622,  0.0204,  0.0773,  0.0617,\n",
       "                       -0.0328, -0.0788, -0.0230,  0.0192,  0.0017,  0.0179,  0.0474, -0.0433,\n",
       "                        0.0846, -0.0539,  0.0023,  0.0779, -0.0395,  0.0290, -0.0604,  0.0514,\n",
       "                       -0.0285, -0.0372,  0.0747,  0.0641, -0.0151,  0.0045,  0.0952,  0.0573,\n",
       "                        0.0993,  0.0629,  0.0677, -0.0345, -0.0194, -0.0507, -0.0584,  0.0420,\n",
       "                       -0.0106,  0.0946, -0.0331,  0.0328, -0.0156, -0.0405,  0.0172,  0.0301,\n",
       "                       -0.0122,  0.0039, -0.0355, -0.0888, -0.0903, -0.0624,  0.0685,  0.0741,\n",
       "                        0.0237, -0.0473, -0.0756,  0.0701, -0.0124,  0.0952,  0.0148,  0.0893,\n",
       "                       -0.0201,  0.0457, -0.0159, -0.0015,  0.0104, -0.0686, -0.0162, -0.0221,\n",
       "                       -0.0288,  0.0902, -0.0991,  0.0999, -0.0100,  0.0623, -0.0955,  0.0530,\n",
       "                        0.0079,  0.0442, -0.0755,  0.0763],\n",
       "                      [ 0.0931,  0.0986, -0.0286,  0.0685, -0.0482, -0.0388, -0.0675,  0.0609,\n",
       "                        0.0152, -0.0359,  0.0117,  0.0344,  0.0560, -0.0793, -0.0510, -0.0042,\n",
       "                        0.0847, -0.0182,  0.0622, -0.0307, -0.0923,  0.0823, -0.0775,  0.0654,\n",
       "                       -0.0267, -0.0072, -0.0874,  0.0473, -0.0674,  0.0146,  0.0358,  0.0138,\n",
       "                        0.0626,  0.0501,  0.0075, -0.0416,  0.0291, -0.0084, -0.0047,  0.0545,\n",
       "                        0.0639,  0.0534, -0.0266,  0.0274,  0.0410, -0.0491, -0.0221,  0.0449,\n",
       "                        0.0559, -0.0063, -0.0229,  0.0118, -0.0403, -0.0856,  0.0340, -0.0051,\n",
       "                        0.0928,  0.0583, -0.0490, -0.0801,  0.0494, -0.0656, -0.0822,  0.0665,\n",
       "                       -0.0834,  0.0336, -0.0034,  0.0853,  0.0503, -0.0076, -0.0820, -0.0219,\n",
       "                        0.0332,  0.0716,  0.0882,  0.0737,  0.0014, -0.0103,  0.0273,  0.0460,\n",
       "                        0.0417,  0.0001,  0.0105, -0.0443,  0.0209, -0.0969, -0.0971,  0.0789,\n",
       "                       -0.0477, -0.0698,  0.0912, -0.0772,  0.0201,  0.0712, -0.0833,  0.0309,\n",
       "                        0.0452, -0.0181, -0.0760, -0.0632],\n",
       "                      [-0.0672,  0.0919,  0.0643,  0.0316,  0.0901,  0.0314, -0.0185,  0.0923,\n",
       "                        0.0573,  0.0605,  0.0082,  0.0448, -0.0730, -0.0970,  0.0524, -0.0096,\n",
       "                        0.0838,  0.0148, -0.0963, -0.0105,  0.0459,  0.0873, -0.0657,  0.0313,\n",
       "                        0.0908, -0.0656, -0.0444, -0.0191, -0.0569, -0.0658,  0.0164, -0.0803,\n",
       "                       -0.0361, -0.0026, -0.0850, -0.0182,  0.0411,  0.0107,  0.0898, -0.0054,\n",
       "                       -0.0441, -0.0007,  0.0392, -0.0133, -0.0446, -0.0165,  0.0068,  0.0444,\n",
       "                        0.0267, -0.0703, -0.0282,  0.0270, -0.0628,  0.0957,  0.0812, -0.0626,\n",
       "                        0.0863,  0.0705, -0.0671, -0.0500, -0.0237,  0.0052,  0.0146, -0.0637,\n",
       "                        0.0300, -0.0645,  0.0379, -0.0536,  0.0179,  0.0537,  0.0923, -0.0241,\n",
       "                       -0.0859, -0.0728,  0.0949, -0.0788, -0.0470,  0.0931,  0.0187,  0.0752,\n",
       "                       -0.0188,  0.0748,  0.0370,  0.0071, -0.0713,  0.0352,  0.0644,  0.0814,\n",
       "                        0.0242, -0.0540,  0.0094, -0.0734, -0.0685,  0.0891,  0.0699,  0.0519,\n",
       "                       -0.0758, -0.0205,  0.0582,  0.0531],\n",
       "                      [ 0.0615,  0.0947, -0.0777, -0.0921,  0.0252,  0.0045,  0.0432, -0.0639,\n",
       "                        0.0394,  0.0322, -0.0455, -0.0684,  0.0241, -0.0905,  0.0505, -0.0892,\n",
       "                       -0.0620,  0.0771, -0.0351, -0.0613, -0.0931, -0.0010, -0.0916, -0.0174,\n",
       "                       -0.0418, -0.0266, -0.0526,  0.0244,  0.0757,  0.0235,  0.0169, -0.0283,\n",
       "                        0.0796,  0.0488,  0.0843,  0.0005,  0.0987,  0.0746,  0.0888,  0.0356,\n",
       "                        0.0854, -0.0730,  0.0364,  0.0222,  0.0997,  0.0830,  0.0190,  0.0537,\n",
       "                       -0.0692, -0.0473, -0.0075,  0.0695, -0.0267,  0.0643, -0.0932,  0.0439,\n",
       "                        0.0914, -0.0343,  0.0524, -0.0425, -0.0633, -0.0100,  0.0352,  0.0993,\n",
       "                        0.0521,  0.0898,  0.0712, -0.0369,  0.0641,  0.0382,  0.0146, -0.0379,\n",
       "                       -0.0743, -0.0078, -0.0660, -0.0794, -0.0778,  0.0504, -0.0510,  0.0494,\n",
       "                       -0.0982,  0.0972, -0.0715, -0.0236, -0.0802, -0.0163, -0.0217,  0.0879,\n",
       "                        0.0905, -0.0169,  0.0320,  0.0121,  0.0056,  0.0710, -0.0132, -0.0519,\n",
       "                       -0.0635, -0.0811,  0.0818, -0.0199],\n",
       "                      [ 0.0593, -0.0639, -0.0828, -0.0835,  0.0632, -0.0035,  0.0596,  0.0089,\n",
       "                       -0.0199,  0.0956, -0.0938,  0.0259, -0.0019,  0.0918, -0.0041,  0.0719,\n",
       "                        0.0751,  0.0141, -0.0424,  0.0832,  0.0115,  0.0950, -0.0979, -0.0501,\n",
       "                       -0.0845,  0.0948, -0.0370, -0.0334, -0.0739, -0.0160, -0.0169, -0.0717,\n",
       "                        0.0299, -0.0268, -0.0175, -0.0646,  0.0468,  0.0456,  0.0574, -0.0064,\n",
       "                       -0.0251,  0.0268,  0.0833, -0.0515, -0.0099, -0.0868, -0.0306,  0.0097,\n",
       "                        0.0850, -0.0962,  0.0774, -0.0468, -0.0003, -0.0619,  0.0625, -0.0618,\n",
       "                       -0.0771,  0.0611,  0.0932, -0.0874,  0.0554, -0.0236, -0.0493,  0.0857,\n",
       "                       -0.0461,  0.0876,  0.0315, -0.0320,  0.0315,  0.0478,  0.0537,  0.0519,\n",
       "                        0.0058,  0.0087, -0.0506,  0.0280,  0.0935, -0.0723,  0.0298, -0.0671,\n",
       "                        0.0297,  0.0834, -0.0874, -0.0385, -0.0085, -0.0594,  0.0397,  0.0073,\n",
       "                       -0.0044,  0.0401, -0.0671, -0.0411, -0.0073, -0.0571, -0.0124, -0.0356,\n",
       "                       -0.0720, -0.0964,  0.0551,  0.0042],\n",
       "                      [-0.0283,  0.0805,  0.0225, -0.0467,  0.0881,  0.0659,  0.0043, -0.0810,\n",
       "                       -0.0335,  0.0507,  0.0044, -0.0836, -0.0193,  0.0387,  0.0912,  0.0740,\n",
       "                        0.0121, -0.0052,  0.0828,  0.0030,  0.0930, -0.0538,  0.0168, -0.0842,\n",
       "                        0.0836, -0.0947,  0.0398,  0.0779, -0.0814, -0.0274,  0.0397, -0.0855,\n",
       "                        0.0745,  0.0281,  0.0610,  0.0216, -0.0680,  0.0044,  0.0520, -0.0351,\n",
       "                       -0.0202,  0.0128,  0.0217,  0.0509,  0.0104,  0.0472,  0.0954, -0.0081,\n",
       "                        0.0939, -0.0990,  0.0719, -0.0205, -0.0279,  0.0209, -0.0496, -0.0672,\n",
       "                        0.0412,  0.0435,  0.0074, -0.0901,  0.0789, -0.0787,  0.0481, -0.0631,\n",
       "                        0.0348,  0.0429, -0.0100,  0.0184,  0.0271,  0.0659,  0.0963,  0.0913,\n",
       "                        0.0737,  0.0786, -0.0688,  0.0093,  0.0214,  0.0166,  0.0976,  0.0101,\n",
       "                       -0.0985, -0.0454,  0.0028, -0.0624, -0.0644, -0.0981, -0.0503, -0.0234,\n",
       "                        0.0057, -0.0229, -0.0940,  0.0249, -0.0168,  0.0800,  0.0224, -0.0106,\n",
       "                       -0.0117, -0.0052,  0.0926,  0.0616],\n",
       "                      [ 0.0383,  0.0051,  0.0971,  0.0749, -0.0969,  0.0534,  0.0082, -0.0455,\n",
       "                        0.0288,  0.0392,  0.0507,  0.0232, -0.0594, -0.0174, -0.0746,  0.0073,\n",
       "                        0.0007,  0.0226, -0.0269,  0.0347, -0.0533, -0.0518,  0.0515,  0.0591,\n",
       "                       -0.0920,  0.0362, -0.0857, -0.0567,  0.0512,  0.0570,  0.0789, -0.0106,\n",
       "                       -0.0046,  0.0386, -0.0280,  0.0698,  0.0020, -0.0510,  0.0576,  0.0673,\n",
       "                        0.0870, -0.0078,  0.0825,  0.0598, -0.0165, -0.0006, -0.0275, -0.0796,\n",
       "                        0.0581,  0.0374,  0.0268, -0.0122,  0.0439,  0.0311,  0.0285,  0.0420,\n",
       "                        0.0540,  0.0695,  0.0516, -0.0503,  0.0578, -0.0764, -0.0251, -0.0352,\n",
       "                       -0.0828, -0.0897, -0.0676,  0.0450, -0.0842,  0.0023, -0.0985, -0.0997,\n",
       "                        0.0325,  0.0009, -0.0985,  0.0938, -0.0286, -0.0712,  0.0181, -0.0437,\n",
       "                       -0.0191,  0.0783,  0.1000,  0.0202, -0.0641, -0.0059,  0.0672, -0.0176,\n",
       "                        0.0876, -0.0965, -0.0195,  0.0541, -0.0550,  0.0699,  0.0045,  0.0662,\n",
       "                        0.0364, -0.0508,  0.0599,  0.0570],\n",
       "                      [ 0.0429, -0.0573, -0.0421, -0.0523, -0.0049,  0.0631, -0.0667, -0.0967,\n",
       "                       -0.0142,  0.0831,  0.0260, -0.0085, -0.0682, -0.0089, -0.0323, -0.0170,\n",
       "                       -0.0961, -0.0272, -0.0848, -0.0983, -0.0645, -0.0059, -0.0597,  0.0541,\n",
       "                        0.0162, -0.0406,  0.0198, -0.0253, -0.0044,  0.0681,  0.0752, -0.0215,\n",
       "                        0.0624,  0.0277,  0.0797, -0.0069,  0.0921, -0.0174,  0.0615,  0.0409,\n",
       "                       -0.0546, -0.0457,  0.0293,  0.0341,  0.0048, -0.0008, -0.0148,  0.0994,\n",
       "                       -0.0016,  0.0615,  0.0980, -0.0867,  0.0703, -0.0561, -0.0003, -0.0378,\n",
       "                       -0.0704, -0.0609,  0.0386, -0.0258,  0.0907,  0.0910,  0.0542,  0.0195,\n",
       "                        0.0206,  0.0625,  0.0142, -0.0405,  0.0829, -0.0896, -0.0987,  0.0861,\n",
       "                       -0.0619,  0.0065,  0.0509, -0.0787,  0.0107, -0.0744,  0.0038,  0.0740,\n",
       "                       -0.0188,  0.0833, -0.0282,  0.0374, -0.0622,  0.0312,  0.0623,  0.0383,\n",
       "                       -0.0218, -0.0062,  0.0467,  0.0966, -0.0397,  0.0144, -0.0984, -0.0147,\n",
       "                        0.0246,  0.0006, -0.0925, -0.0746],\n",
       "                      [-0.0195,  0.0226,  0.0848,  0.0822,  0.0220,  0.0747, -0.0140, -0.0071,\n",
       "                        0.0348, -0.0701, -0.0824,  0.0634,  0.0417, -0.0354,  0.0177, -0.0105,\n",
       "                       -0.0619,  0.0819,  0.0092, -0.0249,  0.0161,  0.0589, -0.0206, -0.0458,\n",
       "                       -0.0773, -0.0371, -0.0513, -0.0004, -0.0203,  0.0463,  0.0509,  0.0828,\n",
       "                        0.0563, -0.0971, -0.0926,  0.0449, -0.0709,  0.0636,  0.0848, -0.0452,\n",
       "                        0.0400, -0.0844,  0.0804, -0.0490,  0.0461,  0.0227,  0.0119,  0.0296,\n",
       "                       -0.0142, -0.0935,  0.0693,  0.0674, -0.0478, -0.0587,  0.0664,  0.0579,\n",
       "                        0.0962,  0.0670,  0.0483,  0.0150, -0.0984,  0.0919,  0.0483,  0.0362,\n",
       "                        0.0184,  0.0931,  0.0738,  0.0732, -0.0996,  0.0460, -0.0534, -0.0529,\n",
       "                       -0.0184, -0.0290, -0.0482, -0.0929, -0.0084,  0.0951, -0.0072, -0.0245,\n",
       "                       -0.0083, -0.0380,  0.0020,  0.0190, -0.0443,  0.0221, -0.0352, -0.0895,\n",
       "                       -0.0241,  0.0265, -0.0747,  0.0793, -0.0716,  0.0058, -0.0671,  0.0026,\n",
       "                        0.0914,  0.0203,  0.0608, -0.0666]]))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0638,  0.0026,  0.2442, -0.1091,  0.0821, -0.0672,  0.2867, -0.0275,\n",
       "         -0.1573,  0.1941]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.ones(1,784)\n",
    "net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "2. https://github.com/CKQu1/extended-criticality-dnn\n",
    "\n",
    "3. Deep Learning, Goodfellow, I., Bengio, Y., Courville A.\n",
    "\n",
    "4. Programming PyTorch for Deep Learning, Pointer, I."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
